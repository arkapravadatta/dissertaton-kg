{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbaf77e",
   "metadata": {},
   "source": [
    "# BASELINE VECTOR-ONLY RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a9058",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe816bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns detected: ['Employee_Name', 'EmpID', 'MarriedID', 'MaritalStatusID', 'GenderID', 'EmpStatusID', 'DeptID', 'PerfScoreID', 'FromDiversityJobFairID', 'Salary', 'Termd', 'PositionID', 'Position', 'State', 'Zip', 'DOB', 'Sex', 'MaritalDesc', 'CitizenDesc', 'HispanicLatino', 'RaceDesc', 'DateofHire', 'DateofTermination', 'TermReason', 'EmploymentStatus', 'Department', 'ManagerName', 'ManagerID', 'RecruitmentSource', 'PerformanceScore', 'EngagementSurvey', 'EmpSatisfaction', 'SpecialProjectsCount', 'LastPerformanceReview_Date', 'DaysLateLast30', 'Absences']\n",
      "Total rows: 311\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV safely\n",
    "df = pd.read_csv(\"HRDataset_v14.csv\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Columns detected:\", list(df.columns))\n",
    "print(\"Total rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7a12e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "Employee_Name: Adinolfi, Wilson  K | EmpID: 10026 | MarriedID: 0 | MaritalStatusID: 0 | GenderID: 1 | EmpStatusID: 1 | DeptID: 5 | PerfScoreID: 4 | FromDiversityJobFairID: 0 | Salary: 62506 | Termd: 0 | PositionID: 19 | Position: Production Technician I | State: MA | Zip: 1960 | DOB: 07/10/83 | Sex: M  | MaritalDesc: Single | CitizenDesc: US Citizen | HispanicLatino: No | RaceDesc: White | DateofHire: 7/5/2011 | TermReason: N/A-StillEmployed | EmploymentStatus: Active | Department: Production        | ManagerName: Michael Albert | ManagerID: 22.0 | RecruitmentSource: LinkedIn | PerformanceScore: Exceeds | EngagementSurvey: 4.6 | EmpSatisfaction: 5 | SpecialProjectsCount: 0 | LastPerformanceReview_Date: 1/17/2019 | DaysLateLast30: 0 | Absences: 1\n"
     ]
    }
   ],
   "source": [
    "def row_to_text(row):\n",
    "    parts = []\n",
    "    for col, val in row.items():\n",
    "        if pd.notna(val):\n",
    "            parts.append(f\"{col}: {val}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "documents = [row_to_text(row) for _, row in df.iterrows()]\n",
    "\n",
    "print(\"Sample document:\")\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c750d",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be30d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 992\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents(documents)\n",
    "print(\"Total chunks created:\", len(chunks))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87e0d7",
   "metadata": {},
   "source": [
    "## Embedding + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0a9ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline FAISS vector store ready (in-memory)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Embedding model (open-source, local)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create in-memory FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "print(\"Baseline FAISS vector store ready (in-memory)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa44316",
   "metadata": {},
   "source": [
    "## Query & retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78393999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved chunks:\n",
      "\n",
      "1. | ManagerID: 2.0 | RecruitmentSource: Employee Referral | PerformanceScore: Exceeds | EngagementSurvey: 4.6 | EmpSatisfaction: 5 | SpecialProjectsCount: 6 | LastPerformanceReview_Date: 2/21/2019 | DaysLateLast30: 0 | Absences: 16\n",
      "2. for Cause | Department: Sales | ManagerName: Lynn Daneault | ManagerID: 21.0 | RecruitmentSource: Employee Referral | PerformanceScore: PIP | EngagementSurvey: 2.0 | EmpSatisfaction: 5 | SpecialProjectsCount: 0 | LastPerformanceReview_Date: 1/28/2019 | DaysLateLast30: 4 | Absences: 7\n",
      "3. | ManagerID: 7.0 | RecruitmentSource: Employee Referral | PerformanceScore: Fully Meets | EngagementSurvey: 4.11 | EmpSatisfaction: 4 | SpecialProjectsCount: 6 | LastPerformanceReview_Date: 2/25/2019 | DaysLateLast30: 0 | Absences: 16\n"
     ]
    }
   ],
   "source": [
    "# Take query from user input\n",
    "query = input(\"Enter your query: \").strip()\n",
    "\n",
    "if not query:\n",
    "    raise ValueError(\"Query cannot be empty\")\n",
    "\n",
    "# Perform similarity search\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\nRetrieved chunks:\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080fdbf",
   "metadata": {},
   "source": [
    "# KG Construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72bc3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import math\n",
    "\n",
    "# -------- Configuration --------\n",
    "NEO4J_URI = \"neo4j+ssc://b931b8dd.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"mW-0f4ShNF394EffXlczpH-5onIZuEsXRFatduQQN5I\" \n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# -------- Data Preparation --------\n",
    "def load_and_clean_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # 1. Clean Dates\n",
    "    date_cols = [\"DateofHire\", \"DateofTermination\", \"DOB\", \"LastPerformanceReview_Date\"]\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    # 2. Fix ID Types (Float -> Int -> String)\n",
    "    # This handles the case where ManagerID is read as 39.0 instead of 39\n",
    "    df[\"EmpID\"] = df[\"EmpID\"].astype(str)\n",
    "    \n",
    "    # Handle nullable ManagerID safely\n",
    "    def clean_manager_id(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        return str(int(x)) # Converts 39.0 -> 39 -> \"39\"\n",
    "    \n",
    "    df[\"ManagerID\"] = df[\"ManagerID\"].apply(clean_manager_id)\n",
    "    df[\"PositionID\"] = df[\"PositionID\"].apply(lambda x: str(int(x)) if pd.notna(x) else None)\n",
    "\n",
    "    # 3. Handle Text NaNs\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # 4. Convert Date Objects to Strings for Neo4j (Neo4j driver handles ISO strings best)\n",
    "    # We create a dictionary list for batch ingestion\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        record = {\n",
    "            \"emp_id\": row[\"EmpID\"],\n",
    "            \"name\": row[\"Employee_Name\"],\n",
    "            \"salary\": float(row[\"Salary\"]) if row[\"Salary\"] else 0.0,\n",
    "            \"position_id\": row[\"PositionID\"],\n",
    "            \"position_title\": row[\"Position\"],\n",
    "            \"dept_id\": str(row[\"DeptID\"]),\n",
    "            \"department\": row[\"Department\"],\n",
    "            \"manager_id\": row[\"ManagerID\"],\n",
    "            \"recruitment_source\": row[\"RecruitmentSource\"],\n",
    "            \"performance_score\": row[\"PerformanceScore\"],\n",
    "            \"hire_date\": row[\"DateofHire\"].date().isoformat() if pd.notna(row[\"DateofHire\"]) else None,\n",
    "            \"term_date\": row[\"DateofTermination\"].date().isoformat() if pd.notna(row[\"DateofTermination\"]) else None,\n",
    "            \"dob\": row[\"DOB\"].date().isoformat() if pd.notna(row[\"DOB\"]) else None,\n",
    "            \"sex\": row[\"Sex\"],\n",
    "            \"race\": row[\"RaceDesc\"],\n",
    "            \"marital_status\": row[\"MaritalDesc\"]\n",
    "        }\n",
    "        records.append(record)\n",
    "        \n",
    "    return records\n",
    "\n",
    "# -------- Cypher Operations --------\n",
    "\n",
    "def create_constraints(driver):\n",
    "    \"\"\"Creates unique constraints to ensure data integrity and speed up MERGE.\"\"\"\n",
    "    queries = [\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.emp_id IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Department) REQUIRE d.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (pos:Position) REQUIRE pos.title IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Source) REQUIRE s.name IS UNIQUE\"\n",
    "    ]\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        for q in queries:\n",
    "            session.run(q)\n",
    "    print(\"Constraints verified.\")\n",
    "\n",
    "def ingest_batch(driver, data):\n",
    "    \"\"\"\n",
    "    Ingests all data in a single optimized batch transaction using UNWIND.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    \n",
    "    // 1. Merge Department\n",
    "    MERGE (d:Department {name: row.department})\n",
    "    \n",
    "    // 2. Merge Position\n",
    "    MERGE (pos:Position {title: row.position_title})\n",
    "    SET pos.position_id = row.position_id\n",
    "    \n",
    "    // 3. Merge Recruitment Source (New Node Type)\n",
    "    MERGE (src:Source {name: row.recruitment_source})\n",
    "\n",
    "    // 4. Merge Person (Central Node)\n",
    "    MERGE (p:Person {emp_id: row.emp_id})\n",
    "    SET \n",
    "        p.name = row.name,\n",
    "        p.salary = row.salary,\n",
    "        p.dob = date(row.dob),\n",
    "        p.sex = row.sex,\n",
    "        p.race = row.race,\n",
    "        p.marital_status = row.marital_status,\n",
    "        p.performance_score = row.performance_score\n",
    "\n",
    "    // 5. Relationships\n",
    "    MERGE (p)-[:WORKS_IN]->(d)\n",
    "    MERGE (p)-[:RECRUITED_FROM]->(src)\n",
    "\n",
    "    // Handle 'HELD_POSITION' with history logic\n",
    "    MERGE (p)-[r:HELD_POSITION]->(pos)\n",
    "    SET \n",
    "        r.from = date(row.hire_date)\n",
    "    // Only set 'to' date if the employee is actually terminated\n",
    "    FOREACH (_ IN CASE WHEN row.term_date IS NOT NULL THEN [1] ELSE [] END |\n",
    "        SET r.to = date(row.term_date)\n",
    "    )\n",
    "    \n",
    "    // 6. Manager Relationship\n",
    "    // We do this inside the same UNWIND using a sub-query match\n",
    "    with p, row\n",
    "    WHERE row.manager_id IS NOT NULL AND row.manager_id <> \"\"\n",
    "    MATCH (m:Person {emp_id: row.manager_id})\n",
    "    MERGE (p)-[:REPORTS_TO]->(m)\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        session.run(query, rows=data)\n",
    "    print(f\"Batch ingestion complete for {len(data)} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21bec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_776\\2907144151.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints verified.\n",
      "Ingesting Graph...\n",
      "Batch ingestion complete for 311 records.\n",
      "Success! Temporal Knowledge Graph created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Main Execution --------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "    \n",
    "    try:\n",
    "        # 1. Prepare Data\n",
    "        print(\"Processing CSV...\")\n",
    "        data = load_and_clean_data(\"HRDataset_v14.csv\")\n",
    "        \n",
    "        # 2. Setup Database\n",
    "        create_constraints(driver)\n",
    "        \n",
    "        # 3. Ingest Data\n",
    "        # Note: If you have >10,000 rows, chunk 'data' into batches of 1000\n",
    "        print(\"Ingesting Graph...\")\n",
    "        ingest_batch(driver, data)\n",
    "        \n",
    "        print(\"Success! Temporal Knowledge Graph created.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
